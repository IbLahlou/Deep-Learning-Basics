{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3e49c3",
   "metadata": {},
   "source": [
    "# Optimisation Algorithm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Algorithm       | Learning Rate Adaptation | Momentum | Adaptive Learning Rate | Suitable for Large Networks | Suitable for Noisy Data | Convergence Speed | Memory Requirements |\n",
    "|-----------------|-------------------------|----------|------------------------|----------------------------|------------------------|-------------------|---------------------|\n",
    "| Stochastic Gradient Descent (SGD) | No          | Yes      | No                     | Yes                        | Yes                    | Slow              | Low                 |\n",
    "| Adam            | Yes                     | Yes      | Yes                    | Yes                        | Yes                    | Fast              | Moderate            |\n",
    "| RMSprop         | Yes                     | No       | Yes                    | Yes                        | Yes                    | Fast              | Moderate            |\n",
    "| AdaDelta        | Yes                     | No       | Yes                    | Yes                        | Yes                    | Fast              | High                |\n",
    "| Adamax          | Yes                     | Yes      | Yes                    | Yes                        | Yes                    | Fast              | Moderate            |\n",
    "| Nadam           | Yes                     | Yes      | Yes                    | Yes                        | Yes                    | Fast              | Moderate            |\n",
    "| AMSGrad         | Yes                     | Yes      | Yes                    | Yes                        | Yes                    | Fast              | Moderate            |\n",
    "| L-BFGS          | No                      | No       | No                     | No                         | Yes                    | Fast              | High                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72abbfa0",
   "metadata": {},
   "source": [
    "### Stochastic Gradiant Descent (SGD):\n",
    "\n",
    "Stochastic Gradient Descent (SGD): SGD is a widely used optimization algorithm for neural networks. It updates the model's parameters based on the gradients computed on mini-batches of training data. SGD is computationally efficient and can handle large datasets.\n",
    "\n",
    "$$\\text{Beta} \\leftarrow \\text{Beta} - \\alpha \\cdot \\nabla J$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f82c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stochastic_gradient_descent_step(Beta, alpha, gradient_J):\n",
    "    Beta -= alpha * gradient_J\n",
    "    return Beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37797f56",
   "metadata": {},
   "source": [
    "### Adaptive Moment Estimation (Adam):\n",
    "\n",
    "Adam: Adam (Adaptive Moment Estimation) is an adaptive optimization algorithm that combines ideas from both momentum and RMSprop. It adjusts the learning rate for each parameter based on the estimates of the first and second moments of the gradients.\n",
    "\n",
    "$$m_i \\leftarrow \\beta_1 \\cdot m_i + (1 - \\beta_1) \\cdot \\nabla J \\\\\n",
    "v_i \\leftarrow \\beta_2 \\cdot v_i + (1 - \\beta_2) \\cdot (\\nabla J)^2 \\\\\n",
    "\\hat{m} \\leftarrow \\frac{m_i}{1 - \\beta_1} \\\\\n",
    "\\hat{v} \\leftarrow \\frac{v_i}{1 - \\beta_2} \\\\\n",
    "\\text{param} \\leftarrow \\text{param} - \\frac{\\alpha \\cdot \\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6d7087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'param1': 0.4990000001, 'param2': -0.7992558632115032}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adam_optimizer(parameters, gradients, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Adam optimizer implementation.\n",
    "\n",
    "    Parameters:\n",
    "    - parameters: A dictionary containing the model parameters to be updated.\n",
    "    - gradients: A dictionary containing the gradients of the parameters.\n",
    "    - learning_rate: The learning rate (alpha) for the optimizer.\n",
    "    - beta1: Exponential decay rate for the first moment estimates.\n",
    "    - beta2: Exponential decay rate for the second moment estimates.\n",
    "    - epsilon: Small value to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "    - updated_parameters: The updated parameters after applying Adam optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize first and second moment estimates\n",
    "    m = {}\n",
    "    v = {}\n",
    "    t = 0  # Time step\n",
    "\n",
    "    for param_name, gradient in gradients.items():\n",
    "        m[param_name] = np.zeros_like(gradient)\n",
    "        v[param_name] = np.zeros_like(gradient)\n",
    "\n",
    "    updated_parameters = {}\n",
    "\n",
    "    for param_name, param_values in parameters.items():\n",
    "        t += 1\n",
    "        gradient = gradients[param_name]\n",
    "\n",
    "        # Update first moment estimate\n",
    "        m[param_name] = beta1 * m[param_name] + (1 - beta1) * gradient\n",
    "\n",
    "        # Update second moment estimate\n",
    "        v[param_name] = beta2 * v[param_name] + (1 - beta2) * gradient ** 2\n",
    "\n",
    "        # Bias-corrected moment estimates\n",
    "        m_hat = m[param_name] / (1 - beta1 ** t)\n",
    "        v_hat = v[param_name] / (1 - beta2 ** t)\n",
    "\n",
    "        # Update parameters\n",
    "        updated_parameters[param_name] = param_values - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "    return updated_parameters\n",
    "\n",
    "# Example usage\n",
    "# Initialize parameters and gradients\n",
    "initial_parameters = {'param1': 0.5, 'param2': -0.8}\n",
    "gradients = {'param1': 0.1, 'param2': -0.3}\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Apply Adam optimization\n",
    "updated_parameters = adam_optimizer(initial_parameters, gradients, learning_rate, beta1, beta2, epsilon)\n",
    "print(updated_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd0e920",
   "metadata": {},
   "source": [
    "### RMSprop\n",
    "\n",
    "RMSprop: RMSprop (Root Mean Square Propagation) is an optimization algorithm that adapts the learning rate for each parameter based on the moving average of squared gradients. It helps in controlling the step size during training and can converge faster than basic SGD.\n",
    "\n",
    "$$\\text{cache}_i \\leftarrow \\text{decay\\_rate} \\cdot \\text{cache}_i + (1 - \\text{decay\\_rate}) \\cdot (\\nabla J)^2 \\\\\n",
    "\\text{param} \\leftarrow \\text{param} - \\frac{\\alpha \\cdot \\nabla J}{\\sqrt{\\text{cache}_i} + \\epsilon} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0979f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'param1': 0.4968377233398313, 'param2': -0.796837722673165}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rmsprop_optimizer(parameters, gradients, learning_rate, decay_rate=0.9, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    RMSprop optimizer implementation.\n",
    "\n",
    "    Parameters:\n",
    "    - parameters: A dictionary containing the model parameters to be updated.\n",
    "    - gradients: A dictionary containing the gradients of the parameters.\n",
    "    - learning_rate: The learning rate (alpha) for the optimizer.\n",
    "    - decay_rate: Decay rate for the moving average of squared gradients.\n",
    "    - epsilon: Small value to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "    - updated_parameters: The updated parameters after applying RMSprop optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize cache for squared gradients\n",
    "    cache = {}\n",
    "\n",
    "    updated_parameters = {}\n",
    "\n",
    "    for param_name, gradient in gradients.items():\n",
    "        if param_name not in cache:\n",
    "            cache[param_name] = np.zeros_like(gradient)\n",
    "\n",
    "        # Update cache for squared gradients\n",
    "        cache[param_name] = decay_rate * cache[param_name] + (1 - decay_rate) * gradient ** 2\n",
    "\n",
    "        # Update parameters\n",
    "        updated_parameters[param_name] = parameters[param_name] - learning_rate * gradient / (np.sqrt(cache[param_name]) + epsilon)\n",
    "\n",
    "    return updated_parameters\n",
    "\n",
    "# Example usage\n",
    "# Initialize parameters and gradients\n",
    "initial_parameters = {'param1': 0.5, 'param2': -0.8}\n",
    "gradients = {'param1': 0.1, 'param2': -0.3}\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "decay_rate = 0.9\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Apply RMSprop optimization\n",
    "updated_parameters = rmsprop_optimizer(initial_parameters, gradients, learning_rate, decay_rate, epsilon)\n",
    "print(updated_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f215b",
   "metadata": {},
   "source": [
    "### Adadelta\n",
    "\n",
    "Adadelta: Adadelta is an extension of Adagrad that addresses its aggressive learning rate decay. It uses the accumulated gradients to adaptively adjust the learning rate, resulting in more stable convergence.\n",
    "\n",
    "$$\n",
    "\\text{accum\\_grad}_i \\leftarrow \\text{decay\\_rate} \\cdot \\text{accum\\_grad}_i + (1 - \\text{decay\\_rate}) \\cdot (\\nabla J)^2 \\\\\n",
    "\\delta \\leftarrow -\\frac{\\sqrt{\\text{accum\\_delta}_i} + \\epsilon}{\\sqrt{\\text{accum\\_grad}_i} + \\epsilon} \\cdot \\nabla J \\\\\n",
    "\\text{accum\\_delta}_i \\leftarrow \\text{decay\\_rate} \\cdot \\text{accum\\_delta}_i + (1 - \\text{decay\\_rate}) \\cdot \\delta^2 \\\\\n",
    "\\text{param} \\leftarrow \\text{param} + \\delta \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74901b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'param1': 0.49968377381511014, 'param2': -0.7996837724096652}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adadelta_optimizer(parameters, gradients, decay_rate=0.9, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Adadelta optimizer implementation.\n",
    "\n",
    "    Parameters:\n",
    "    - parameters: A dictionary containing the model parameters to be updated.\n",
    "    - gradients: A dictionary containing the gradients of the parameters.\n",
    "    - decay_rate: Decay rate for the moving average of squared gradients and deltas.\n",
    "    - epsilon: Small value to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "    - updated_parameters: The updated parameters after applying Adadelta optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize accumulators for squared gradients and deltas\n",
    "    accum_grad = {}\n",
    "    accum_delta = {}\n",
    "\n",
    "    updated_parameters = {}\n",
    "\n",
    "    for param_name, gradient in gradients.items():\n",
    "        if param_name not in accum_grad:\n",
    "            accum_grad[param_name] = np.zeros_like(gradient)\n",
    "            accum_delta[param_name] = np.zeros_like(gradient)\n",
    "\n",
    "        # Update accumulator for squared gradients\n",
    "        accum_grad[param_name] = decay_rate * accum_grad[param_name] + (1 - decay_rate) * gradient ** 2\n",
    "\n",
    "        # Compute update step\n",
    "        delta = -np.sqrt(accum_delta[param_name] + epsilon) / np.sqrt(accum_grad[param_name] + epsilon) * gradient\n",
    "\n",
    "        # Update accumulator for squared deltas\n",
    "        accum_delta[param_name] = decay_rate * accum_delta[param_name] + (1 - decay_rate) * delta ** 2\n",
    "\n",
    "        # Update parameters\n",
    "        updated_parameters[param_name] = parameters[param_name] + delta\n",
    "\n",
    "    return updated_parameters\n",
    "\n",
    "# Example usage\n",
    "# Initialize parameters and gradients\n",
    "initial_parameters = {'param1': 0.5, 'param2': -0.8}\n",
    "gradients = {'param1': 0.1, 'param2': -0.3}\n",
    "\n",
    "# Set hyperparameters\n",
    "decay_rate = 0.9\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Apply Adadelta optimization\n",
    "updated_parameters = adadelta_optimizer(initial_parameters, gradients, decay_rate, epsilon)\n",
    "print(updated_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4177aa3",
   "metadata": {},
   "source": [
    "### Adamax\n",
    "\n",
    "Adamax: Adamax is a variant of Adam that uses the infinity norm (maximum absolute value) of the gradient instead of the L2 norm. It is more robust to sparse gradients and can be effective in large-scale applications.\n",
    "\n",
    "$$m_i \\leftarrow \\beta_1 \\cdot m_i + (1 - \\beta_1) \\cdot \\nabla J \\\\\n",
    "u_i \\leftarrow \\max(\\beta_2 \\cdot u_i, |\\nabla J|) \\\\\n",
    "\\text{param} \\leftarrow \\text{param} - \\frac{\\alpha \\cdot m_i}{u_i + \\epsilon} \\\\\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019ceeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'param1': 0.49990000001, 'param2': -0.7999000000033334}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adamax_optimizer(parameters, gradients, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Adamax optimizer implementation.\n",
    "\n",
    "    Parameters:\n",
    "    - parameters: A dictionary containing the model parameters to be updated.\n",
    "    - gradients: A dictionary containing the gradients of the parameters.\n",
    "    - learning_rate: The learning rate (alpha) for the optimizer.\n",
    "    - beta1: Exponential decay rate for the first moment estimates.\n",
    "    - beta2: Exponential decay rate for the weighted infinity norm.\n",
    "    - epsilon: Small value to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "    - updated_parameters: The updated parameters after applying Adamax optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize first moment estimates and weighted infinity norm\n",
    "    m = {}\n",
    "    u = {}\n",
    "\n",
    "    t = 0  # Time step\n",
    "\n",
    "    for param_name, gradient in gradients.items():\n",
    "        m[param_name] = np.zeros_like(gradient)\n",
    "        u[param_name] = np.zeros_like(gradient)\n",
    "\n",
    "    updated_parameters = {}\n",
    "\n",
    "    for param_name, param_values in parameters.items():\n",
    "        t += 1\n",
    "        gradient = gradients[param_name]\n",
    "\n",
    "        # Update first moment estimate\n",
    "        m[param_name] = beta1 * m[param_name] + (1 - beta1) * gradient\n",
    "\n",
    "        # Update weighted infinity norm\n",
    "        u[param_name] = np.maximum(beta2 * u[param_name], np.abs(gradient))\n",
    "\n",
    "        # Update parameters\n",
    "        updated_parameters[param_name] = param_values - learning_rate * m[param_name] / (u[param_name] + epsilon)\n",
    "\n",
    "    return updated_parameters\n",
    "\n",
    "# Example usage\n",
    "# Initialize parameters and gradients\n",
    "initial_parameters = {'param1': 0.5, 'param2': -0.8}\n",
    "gradients = {'param1': 0.1, 'param2': -0.3}\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Apply Adamax optimization\n",
    "updated_parameters = adamax_optimizer(initial_parameters, gradients, learning_rate, beta1, beta2, epsilon)\n",
    "print(updated_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997525ee",
   "metadata": {},
   "source": [
    "### Nadam\n",
    "\n",
    "Nadam: Nadam is a combination of Nesterov accelerated gradient (NAG) and Adam. It incorporates the benefits of NAG's momentum and Adam's adaptive learning rate to provide faster convergence and better generalization.\n",
    "\n",
    "$$m_i \\leftarrow \\beta_1 \\cdot m_i + (1 - \\beta_1) \\cdot \\nabla J \\\\\n",
    "v_i \\leftarrow \\beta_2 \\cdot v_i + (1 - \\beta_2) \\cdot (\\nabla J)^2 \\\\\n",
    "\\hat{m} \\leftarrow \\frac{m_i}{1 - \\beta_1} \\\\\n",
    "\\hat{v} \\leftarrow \\frac{v_i}{1 - \\beta_2} \\\\\n",
    "\\text{param} \\leftarrow \\text{param} - \\frac{\\alpha \\cdot (\\beta_1 \\cdot \\hat{m} + (1 - \\beta_1) \\cdot \\nabla J)}{\\sqrt{\\hat{v}} + \\epsilon} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e495b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'param1': 0.4990000001, 'param2': -0.7991888909005386}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nadam_optimizer(parameters, gradients, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Nadam optimizer implementation.\n",
    "\n",
    "    Parameters:\n",
    "    - parameters: A dictionary containing the model parameters to be updated.\n",
    "    - gradients: A dictionary containing the gradients of the parameters.\n",
    "    - learning_rate: The learning rate (alpha) for the optimizer.\n",
    "    - beta1: Exponential decay rate for the first moment estimates.\n",
    "    - beta2: Exponential decay rate for the second moment estimates.\n",
    "    - epsilon: Small value to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "    - updated_parameters: The updated parameters after applying Nadam optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize first and second moment estimates\n",
    "    m = {}\n",
    "    v = {}\n",
    "    t = 0  # Time step\n",
    "\n",
    "    for param_name, gradient in gradients.items():\n",
    "        m[param_name] = np.zeros_like(gradient)\n",
    "        v[param_name] = np.zeros_like(gradient)\n",
    "\n",
    "    updated_parameters = {}\n",
    "\n",
    "    for param_name, param_values in parameters.items():\n",
    "        t += 1\n",
    "        gradient = gradients[param_name]\n",
    "\n",
    "        # Update first moment estimate\n",
    "        m[param_name] = beta1 * m[param_name] + (1 - beta1) * gradient\n",
    "\n",
    "        # Update second moment estimate\n",
    "        v[param_name] = beta2 * v[param_name] + (1 - beta2) * gradient ** 2\n",
    "\n",
    "        # Bias-corrected moment estimates\n",
    "        m_hat = m[param_name] / (1 - beta1 ** t)\n",
    "        v_hat = v[param_name] / (1 - beta2 ** t)\n",
    "\n",
    "        # Update parameters\n",
    "        updated_parameters[param_name] = param_values - learning_rate * ((beta1 * m_hat) + ((1 - beta1) * gradient)) / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "    return updated_parameters\n",
    "\n",
    "# Example usage\n",
    "# Initialize parameters and gradients\n",
    "initial_parameters = {'param1': 0.5, 'param2': -0.8}\n",
    "gradients = {'param1': 0.1, 'param2': -0.3}\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Apply Nadam optimization\n",
    "updated_parameters = nadam_optimizer(initial_parameters, gradients, learning_rate, beta1, beta2, epsilon)\n",
    "print(updated_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12633448",
   "metadata": {},
   "source": [
    "### AdaGrad-Delta\n",
    "\n",
    "AdaGrad-Delta: AdaGrad-Delta is an extension of Adagrad that addresses its rapid learning rate decay. It introduces a decaying sum of squared gradients to adapt the learning rate and control the step size more effectively.\n",
    "\n",
    "$$\\text{cache}_i \\leftarrow \\text{cache}_i + (\\nabla J)^2 \\\\\n",
    "\\delta_i \\leftarrow -\\frac{\\sqrt{\\delta_i} + \\epsilon}{\\sqrt{\\text{cache}_i} + \\epsilon} \\cdot \\nabla J \\\\\n",
    "\\text{param} \\leftarrow \\text{param} + \\delta_i \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f4690c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'param1': 0.49990000004999996, 'param2': -0.7999000000055556}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adagrad_delta_optimizer(parameters, gradients, learning_rate, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    AdaGrad-Delta optimizer implementation.\n",
    "\n",
    "    Parameters:\n",
    "    - parameters: A dictionary containing the model parameters to be updated.\n",
    "    - gradients: A dictionary containing the gradients of the parameters.\n",
    "    - learning_rate: The learning rate (alpha) for the optimizer.\n",
    "    - epsilon: Small value to prevent division by zero.\n",
    "\n",
    "    Returns:\n",
    "    - updated_parameters: The updated parameters after applying AdaGrad-Delta optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize cache for squared gradients and deltas\n",
    "    cache = {}\n",
    "    delta = {}\n",
    "\n",
    "    updated_parameters = {}\n",
    "\n",
    "    for param_name, gradient in gradients.items():\n",
    "        if param_name not in cache:\n",
    "            cache[param_name] = np.zeros_like(gradient)\n",
    "            delta[param_name] = np.zeros_like(gradient)\n",
    "\n",
    "        # Update cache for squared gradients\n",
    "        cache[param_name] += gradient ** 2\n",
    "\n",
    "        # Compute update step\n",
    "        delta[param_name] = -np.sqrt(delta[param_name] + epsilon) / np.sqrt(cache[param_name] + epsilon) * gradient\n",
    "\n",
    "        # Update parameters\n",
    "        updated_parameters[param_name] = parameters[param_name] + delta[param_name]\n",
    "\n",
    "    return updated_parameters\n",
    "\n",
    "# Example usage\n",
    "# Initialize parameters and gradients\n",
    "initial_parameters = {'param1': 0.5, 'param2': -0.8}\n",
    "gradients = {'param1': 0.1, 'param2': -0.3}\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Apply AdaGrad-Delta optimization\n",
    "updated_parameters = adagrad_delta_optimizer(initial_parameters, gradients, learning_rate, epsilon)\n",
    "print(updated_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3d35a",
   "metadata": {},
   "source": [
    "### L-BFGS\n",
    "\n",
    "L-BFGS: L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) is a quasi-Newton optimization algorithm that approximates the Hessian matrix. It can be efficient for small-scale problems but may not scale well to large neural networks.\n",
    "\n",
    "From an initial guess $\\mathbf{x}_0$ and an approximate inverted Hessian matrix $H_0$, the following steps are repeated as $\\mathbf{x}_k$ converges to the solution:\n",
    "\n",
    "1. Obtain a direction $\\mathbf{p}_k$ by solving $\\mathbf{p}_k = -H_k \\nabla J(\\mathbf{x}_k)$, where $\\nabla J(\\mathbf{x}_k)$ is the gradient of the function at $\\mathbf{x}_k$.\n",
    "2. Perform a one-dimensional optimization (line search) to find an acceptable step size $\\alpha_k$ in the direction found in the first step. If an exact line search is performed, then $\\alpha_k = \\arg \\min J(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$. In practice, an inexact line search usually suffices, with an acceptable $\\alpha_k$ satisfying the Wolfe conditions.\n",
    "3. Set $\\mathbf{s}_k = \\alpha_k \\mathbf{p}_k$ and update $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\mathbf{s}_k$.\n",
    "4. Compute $\\mathbf{y}_k = \\nabla J(\\mathbf{x}_{k+1}) - \\nabla J(\\mathbf{x}_k)$.\n",
    "5. Update the approximate inverted Hessian matrix $H_{k+1} = H_k + \\Delta H_k$, where:\n",
    "   $$\\Delta H_k = \\frac{(\\mathbf{s}_k^T \\mathbf{y}_k + \\mathbf{y}_k^T H_k \\mathbf{y}_k)(\\mathbf{s}_k \\mathbf{s}_k^T)}{(\\mathbf{s}_k^T \\mathbf{y}_k)^2} - \\frac{H_k \\mathbf{y}_k \\mathbf{s}_k^T + \\mathbf{s}_k \\mathbf{y}_k^T H_k}{\\mathbf{s}_k^T \\mathbf{y}_k} + \\gamma I$$.\n",
    "   \n",
    "Here, $\\Delta H_k$ is the correction term to update the Hessian approximation, $\\mathbf{s}_k$ is the difference in parameter vectors, $\\mathbf{y}_k$ is the difference in gradient vectors, $\\gamma$ is a regularization parameter, and $I$ is the identity matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e55f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def lbfgs_optimizer(objective_function, initial_parameters, max_iterations=100, tolerance=1e-6):\n",
    "    def objective_wrapper(parameters):\n",
    "        value, gradient = objective_function(parameters)\n",
    "        return value, gradient\n",
    "\n",
    "    optimized_result = minimize(\n",
    "        objective_wrapper,\n",
    "        initial_parameters,\n",
    "        jac=True,\n",
    "        method='L-BFGS-B',\n",
    "        options={'maxiter': max_iterations, 'ftol': tolerance}\n",
    "    )\n",
    "    return optimized_result\n",
    "\n",
    "# Define your objective function\n",
    "def objective_function(parameters):\n",
    "    x, y = parameters\n",
    "    value = (x - 2) ** 2 + (y + 3) ** 2  # Example quadratic function\n",
    "    gradient = np.array([2 * (x - 2), 2 * (y + 3)])  # Gradient of the function\n",
    "    return value, gradient\n",
    "\n",
    "# Example usage\n",
    "initial_parameters = np.array([0.0, 0.0])  # Initial guess for parameters\n",
    "\n",
    "# Use the L-BFGS optimizer to find the optimal parameters\n",
    "optimized_result = lbfgs_optimizer(objective_function, initial_parameters)\n",
    "\n",
    "# Print the results\n",
    "print(\"Optimized parameters:\", optimized_result.x)\n",
    "print(\"Optimization status:\", optimized_result.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb7f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
